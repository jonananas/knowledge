# Let's build GPT: from scratch, in code, spelled out.

Paper: Attention Is All You Need

Transformer based neural network - a language model

Dataset: Tiny Shakespeare

https://github.com/karpathy/nanoGPT

https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing

https://github.com/karpathy/ng-video-lecture


We went through the example code together in order to understand what it does.

We learned details about creating a Transformer network, such as
- encode/decode to/from tokens
- split dataset into training and validation sets
- BigramLanguageModel
- What is a tensor?
- What is cross entropy
- encoder, decoder, tensor, block and batch parameters
- Self attention
- Cross attention
- Single and multi-head attention

We also understood that "For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers."

We suggest that After reading and discussing the "What Is ChatGPT Doing â€¦ and Why Does It Work?" article, it would be worth to revisit the article the video is built on, "Attention Is All You Need"